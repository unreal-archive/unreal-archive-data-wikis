---
parse:
  title: "Legacy:Chongqing Page/Discuss"
  text:
    text: "<p>There are two types of \"bans\" (actually, read-only states) imposed\
      \ by the automatic spam filter:</p>\n<ul>\n<li><b><a href=\"/Legacy:Chongqing_Page/Temporary_Read-Only\"\
      \ title=\"Legacy:Chongqing Page/Temporary Read-Only\">Temporary Bans</a></b>\
      \ are imposed for adding several, but not too many links to a page. The page\
      \ (with the links) is saved, and the user is put on read-only for ten minutes\
      \ for each link he or she added.</li>\n</ul>\n<ul>\n<li><b><a href=\"/Legacy:Chongqing_Page/Permanent_Read-Only\"\
      \ title=\"Legacy:Chongqing Page/Permanent Read-Only\">Permanent Bans</a></b>\
      \ are imposed for adding too many links to a page. The changes to the page are\
      \ discarded, and the user is put on read-only permanently.</li>\n</ul>\n<p>I'm\
      \ intentionally keeping the exact number of links required to trigger either\
      \ of those cases vague. I'd rather not have spammers fine-tune their spamming\
      \ attempts using that information. &#160;;-)</p>\n<p>In both cases, an email\
      \ containing time, IP address, modified page name and submitted page content\
      \ is dispatched to the <a href=\"/Legacy:Wiki_Admin\" title=\"Legacy:Wiki Admin\"\
      >wiki admins</a>. This allows them to react quickly to temporary bans (to revert\
      \ the changes and make the ban permanent if a spammer was caught, or to undo\
      \ the automatic ban on false positives), and it serves as proof when filing\
      \ complaints to the spammers' ISPs.</p>\n<p>If somebody would like to be added\
      \ to the list of users receiving those notification emails, drop <a href=\"\
      /Legacy:Mychaeel\" title=\"Legacy:Mychaeel\">Mychaeel</a> a line.</p>\n<p>See\
      \ also <a href=\"/Legacy:Chongqing_Page/Statistics\" title=\"Legacy:Chongqing\
      \ Page/Statistics\">Chongqing Page/Statistics</a>.</p>\n<h2><span class=\"mw-headline\"\
      \ id=\"Discussion\">Discussion</span><span class=\"mw-editsection\"><span class=\"\
      mw-editsection-bracket\">[</span><a href=\"/edit/Legacy:Chongqing_Page/Discuss?section=1\"\
      \ title=\"Edit section: Discussion\">edit</a><span class=\"mw-editsection-bracket\"\
      >]</span></span></h2>\n<p><b>Tarquin:</b> Just for added fun, I have hacked\
      \ our page database so even the old revision of the page this guy edited leads\
      \ to chongqed.org bwahahahaha&#160;:D</p>\n<p><b>RavuAlHemio:</b>&#160;:D You're\
      \ kicking the spammers anywhere you can, huh?&#160;:D</p>\n<p><b>anonymous:</b>\
      \ Yes indeed folks go on treating them *******s the hard way. I had real troubles\
      \ with spammers myself glad to see some action</p>\n<p><b>Foxpaw:</b> The ones\
      \ I just posted didn't really have any keywords, not ones legible on my PC,\
      \ anyway, so I just replaced the keywords with the URL of the sites..</p>\n\
      <p><b>MythOpus:</b> This spamming is making me mad. Grr. I suggest we keep it\
      \ open to the public to edit as they see fit, but I think we should have people\
      \ sign up for the privelage. We could really make use of the passwords in the\
      \ preferences menu for this and it will help with the spam problem and if it\
      \ continues then we could easilly find out EXACTLY who is doing it. Of course,\
      \ that may go against what this wiki was meant for but still... Desperate Times,\
      \ Desperate Measures.</p>\n<p><b>Graphik:</b> Hello Myth.&#160;:)</p>\n<p>Times\
      \ aren't desperate, just annoying.&#160;;) I thought of the same thing, but\
      \ in the end it is like you said, that's not what a wiki is meant for. Eventually\
      \ they'll give up; it's very unsatisfying to see your work undone again and\
      \ again. We already can find out who it is without registration, and I just\
      \ banned one person today.&#160;:)</p>\n<p><b>MythOpus:</b> Note to all Spammers...\
      \ Bow now, or bow later... muhahahahaphmuhahaMPH!!</p>\n<p><b>Mychaeel:</b>\
      \ I don't really believe in technical means to stop spammers, but since those\
      \ asian spam links are easy enough to spot and never used anywhere else on the\
      \ Wiki, I've added a bit of code to the script that redirects people trying\
      \ to save a page which contains such a link. Try it yourself... &#160;:-)</p>\n\
      <p><b>Foxpaw:</b> The spamming has been intense lately. Is there a known reason\
      \ for the surge in spamming? I don't remember it ever happening less than a\
      \ year ago, not it seems within the last week or so it's been several spamvertizements\
      \ a day.</p>\n<p><b>Mychaeel:</b> I think the one and only reason is that spammers\
      \ have discovered wikis. Now, true to their \"I'm the only thing that matters\
      \ in the world\" attitude, they hook onto wikis everywhere in order to milk\
      \ them for whatever benefit they see in doing so until they're dead and useless\
      \ – like a deadly virus. Fortunately for us, their limited view of the world\
      \ doesn't quite match reality, so there's still hope for the social rest of\
      \ the world.</p>\n<p><b>Tarquin:</b> Does Google's bot see old revisions of\
      \ pages?</p>\n<p><b>Mychaeel:</b> Our <a rel=\"nofollow\" class=\"external text\"\
      \ href=\"http://wiki.beyondunreal.com/robots.txt\">robots.txt</a> wasn't properly\
      \ set up to prevent this when I checked (only disallowing access to <code>/cgi-bin/wiki.pl?action=</code>,\
      \ not <code>/wiki?action=</code>). I've fixed this.</p>\n<p><b>OverloadUT:</b>\
      \ I noticed that if you check the \"minor edit\" box when editing a page, it\
      \ does not jump to the top of the list on the <a href=\"/Legacy:Recent_Changes\"\
      \ title=\"Legacy:Recent Changes\" class=\"mw-redirect\">Recent Changes</a> page.\
      \ If a spammer figured this out, couldn't they check that box on all their spammed\
      \ pages, and it would take much longer to find them? Or is there some sort of\
      \ protection against this?</p>\n<p><b>Tarquin:</b> Yes: set your preferences\
      \ to see minor edits!&#160;:)</p>\n<p><b>Mychaeel:</b> The <a href=\"/edit/Localcgi:chongqme?redlink=1\"\
      \ class=\"new\" title=\"Localcgi:chongqme (page does not exist)\">chongqing\
      \ tool</a> I added today makes is pretty simple to chongqify stuff added by\
      \ a spammer; however, it doesn't register those keywords at <a rel=\"nofollow\"\
      \ class=\"external free\" href=\"http://chongqed.org\">http://chongqed.org</a>.\
      \ The chongqing links still do their purpose, but they lead to an unimpressive\
      \ \"The keyword [...] does not seem to be in the database\" page.</p>\n<p><b>Foxpaw:</b>\
      \ Just thought of something - this page is going to get huge, and sufficiently\
      \ huge pages seem to time out when you try to save them - that could become\
      \ a problem as this page grows. The chongquing tool was very handy though, and\
      \ just in time, as this spamming binge was enormous.</p>\n<p><b>Parallax:</b>\
      \ Pagerank on google is based on the number of links into the linking page.\
      \ We should have every single page in the wiki containing a link to this Chongqing\
      \ page so that it gets a very very high page rank. Such a link to this page\
      \ could be very discreet. That would mean that this page would have more authority\
      \ when it comes to destroying the page rank of the spammers.</p>\n<p><b>Tarquin:</b>\
      \ Yup. We could add it to the footer (in a small font) or something.</p>\n<p><b>Foxpaw:</b>\
      \ Not only would that greatly increase loading times for pages on the wiki,\
      \ but I don't think that it would work. Search engine spiders probrably only\
      \ recognize a given link/keyword combination once per domain - otherwise spammers\
      \ wouldn't have to spam wikis and blogs, they could just spam one domain (that\
      \ they controlled) a whole ton of times.</p>\n<p><b>Mychaeel:</b> I don't think\
      \ anybody's talking about adding the links <i>on</i> this page to the footer\
      \ – it's about adding a link <i>to</i> this page to the footer. That really\
      \ wouldn't increase loading times a lot.</p>\n<p><b>Wormbo:</b> I've just removed\
      \ over 50% of the links because they were duplicates. Please check that first\
      \ if you add them so we can keep the page size within reasonable limits. Does\
      \ linking with an URL as the link text actually help?</p>\n<p><b>Halz</b> Yeah\
      \ those URL links under 'Miscellaneous' are less effective chongqing links,\
      \ particularly as they all point to the same page, rather than specific parts\
      \ of the chongqed.org database. Also the links like <a rel=\"nofollow\" class=\"\
      external text\" href=\"http://spammers.chongqed.org/Effexor%20and%20weight%20loss\"\
      >Effexor and weight loss</a> are keywords which you haven't told chongqed.org\
      \ about. To chongq effectively, you have to sumbit the spammer to chongqed.org,\
      \ wait for the keywords to be accepted into the database, and then link using\
      \ the keywords. So a link like <a rel=\"nofollow\" class=\"external text\" href=\"\
      http://spammers.chongqed.org/business%20transcription\">Business Transcription</a>\
      \ is best. However any links to <a rel=\"nofollow\" class=\"external text\"\
      \ href=\"http://www.chongqed.org\">chongqed.org</a> will help boost its general\
      \ ranking. Also as Foxpaw was saying, links from various different domains is\
      \ better than lots of links from just one page, so if you folks have your own\
      \ homepage, put a link there too!</p>\n<p><b>Savannahlion:</b> Yep, I've been\
      \ Chongqed. Initially, this &lt;insert appropriate insult here&gt; started by\
      \ dumping a few links in the Sandbox. Simple enough, I simply banned him. Problem\
      \ solved for a few months.</p>\n<p>Then he returns, and creates two pages in\
      \ his interest. Deleted them. A few days later he assaults about eight or so\
      \ pages with piles of his links. I call him Mr. Seo, as a hint. Unfortunately,\
      \ the tactic of simply Chngqing his links won't work too well. He's creating\
      \ wiki links within existing text without regard for context. Links within code\
      \ where the [ and ] would be misinterpreted. A spam link within the words Counter-Strike,\
      \ which is woefully outside of the context. Even a link to his spam within a\
      \ reference to Google. Completely random words.</p>\n<p>I created a patch to\
      \ prevent his lame additions. This was before I realized Tarquin solved the\
      \ problem but after 4 hours of site clean up, coding, patching, and testing\
      \ and 16 hours at work.</p>\n<p>I'm going to look intto creating a Chongq page\
      \ to support all of this. But the details of implemenation are kind of hazy.\
      \ I'm going to have to mull over implementation details after work. Thank you\
      \ Tarquin for pointing me in the right direction. But I would still like to\
      \ pick your brain personally. IRC, ICQ, something.</p>\n<p><b>Tarquin:</b> A\
      \ Chongq page simply links to the Chongq site to damage the spammer's google\
      \ rating. I'm not in a position at the moment to be often on IRC, so you're\
      \ best off reading up at the Chongq site to see how it works, or catching another\
      \ wiki admin on our channel. Good luck dealing with this guy. Does his IP change\
      \ a lot? You can always try banning him. Or you can hack to wiki code to refuse\
      \ to save edits containing certain words or URLs.</p>\n<hr />\n<p><b>Zxanphorian:</b>\
      \ OMG to all of those sites!</p>\n<p><b>Tarquin:</b> I'm thinking we should\
      \ perhaps post on BuF to ask peopel to help the wiki fight spammers by putting\
      \ a link to this page on their websites. What do you all think?</p>\n<p><b>Foxpaw:</b>\
      \ Hrmm. That would boost the pagerank of this page, but not of Chongqed.org.\
      \ If possible, copying the text from this page to a google-indexed page on their\
      \ own site would probrably work better. That way Chongqed.org gets a higher\
      \ cumulative rank as opposed to those referral links being divided among two\
      \ sites (here and Chongqed.org).</p>\n<p>At least, I don't THINK that being\
      \ google has a heirarchial sort of system where a link to this page would be\
      \ recognized as an indirect link to Chongqed.org.</p>\n<p><b>Tarquin:</b> We're\
      \ not trying to raise the rating of Chongqed.org, we're trying to connect the\
      \ keywords the spammers care about with the link to Chongqed.org. So yeah, copying\
      \ this content works too, but according to the guys at Chongqed.org, making\
      \ this page highly-linked to will be good too.</p>\n<p><b>Foxpaw:</b> Err, well,\
      \ yes, but like I said, I don't think that linking here from elsewhere will\
      \ do anything except raise the pagerank of this page, with the keywords used\
      \ to link here. I wouldn't expect it to have any effect on the pagerank of Chongqed.org\
      \ in combination with the keywords here, as I don't think that google counts\
      \ indirect references like that.</p>\n<p>However, if they say it will work,\
      \ I'll take their word for it.</p>\n<p><b>ElMuerte:</b> we need some magic for\
      \ this page, e.g.:</p>\n<ul>\n<li>links on this page are read from an arbitrary\
      \ file</li>\n<li>to add new chongqing links use an addition script, this script\
      \ will:\n<ul>\n<li>check for duplicates</li>\n<li>... do more? ...</li>\n</ul>\n\
      </li>\n<li>Clearly mark the link to the add script, like: \"If you want to add\
      \ spam to this site, please use the following link\"</li>\n</ul>\n<p>This makes\
      \ things easier:</p>\n<ul>\n<li>no duplicates</li>\n<li>no high loading/saving/editing\
      \ time for this page</li>\n<li>easier to catch new spam on this page</li>\n\
      </ul>\n<p><b>Mychaeel:</b> I was thinking about extending the <a href=\"/edit/Localcgi:chongqme?redlink=1\"\
      \ class=\"new\" title=\"Localcgi:chongqme (page does not exist)\">Chongq My\
      \ Links!</a> script with duplicate-removal functionality – I just didn't get\
      \ around to doing it yet. (Now I've got my computer set up at home again, so\
      \ chances are rising that I'll find the time soon.) – Anyway, I find it amusing\
      \ that this page is being the main target for spammers these days... &#160;:p</p>\n\
      <p><b>Foxpaw:</b> It seems like the logical page to spam, if you think about\
      \ it. Not only do you get the normal \"benefit\" of posting your links, but\
      \ you remove any of your links that may have been chongqed previously too.</p>\n\
      <p><b>Mychaeel:</b> True... though that explanation implies that spammers were\
      \ that smart, which appears hardly plausible to me (just look at the recent\
      \ spam attempts – the link tags were so incompetently crafted that they didn't\
      \ even work). – Maybe this sentence from the edit page header is what makes\
      \ spammers paste their links here: \"Note that old page revisions aren't indexed\
      \ by Google, but the Chongqing Page is.\"</p>\n<p><b>Joe@Chongqed:</b> Hi. You\
      \ guys are doing some good chongqing here, thanks. I just wanted to add to what\
      \ Tarquin said we said about linking (which I had forgot myself). By increasing\
      \ the PageRank for this page it gives the links here more credibility and should\
      \ help their PageRanks too. Links from a page with higher PageRank are more\
      \ valuable. As mentioned above, links from your footer or sidebar on every page\
      \ would help the PageRank of this page. It would also help the PageRank of <a\
      \ rel=\"nofollow\" class=\"external text\" href=\"http://chongqed.org\">chongqed.org</a>\
      \ if you linked from every page, but the purpose of chongqed.org is to remove\
      \ junk from wikis. While more links are great we don't want to clutter up people's\
      \ wikis so are happy with how you guys are doing it.</p>\n<p>I also want to\
      \ invite you guys to our <a rel=\"nofollow\" class=\"external text\" href=\"\
      http://wiki.chongqed.org\">chongqed wiki</a> in case you missed it, its a newer\
      \ addition. We use it to discuss chongqing ideas, spammers, antispam protection,\
      \ etc. It has been up long enough to attract spammers though, it shows how really\
      \ stupid spammers are that they attack chongqed.org, not out of revenge, just\
      \ regular spamming. We have been learning a lot more about spammers recently.\
      \ So far all the ones that have attacked us have found the pages by searching\
      \ for other spammer's URLs. They let the other spammers do the dirty work of\
      \ finding pages where their spam will stay long enough for Google to see it.\
      \ So by keeping a clean wiki you should attract fewer spammers. Because we list\
      \ lots of spammer URLs (though not links) our page appears to spammers to be\
      \ a good target. Although we hate spam, we like being spammed on our wiki, it\
      \ makes chongqing them that much easier and more fun.</p>\n<p><b>Graphik:</b>\
      \ Thanks for the information on your fine service.</p>\n<p>I tried to post this\
      \ once before, but there was an edit conflict; this page was spammed.&#160;:rolleyes:</p>\n\
      <p>I'm not sure if the links I reverted should be chongqued from here. They\
      \ were rather 'adult' in nature and might violate BU's 'no links to porn' hosting\
      \ condition (In spirit, anyway).</p>\n<p><b>Mychaeel:</b> Chongqed links point\
      \ to <a rel=\"nofollow\" class=\"external free\" href=\"http://chongqed.org\"\
      >http://chongqed.org</a>, not porn (unless Chongqed.org has changed its agenda\
      \ since last time I checked). So just chongq ahead!</p>\n<p><b>Graphik:</b>\
      \ Done. Damn this page is going to be long after a while, so even my cable connection\
      \ (much more a dial-up user) will start to struggle with it. I suggest that\
      \ we create a subpage to contain the spammed links. The Chongq My Links! tool\
      \ can automagically add them to the subpage instead of linking back to here\
      \ to add them. &lt;insert better-thought-out variant of that idea here&gt;</p>\n\
      <p><b>Mychaeel:</b> Moved the discussion to a subpage.</p>\n<p><b>Graphik:</b>\
      \ An excellent idea, but that doesn't solve the problem of having to load the\
      \ parent page to chongq links.</p>\n<p><b>Mychaeel:</b> Right, but neither would\
      \ keeping the discussion on the main page and putting the links on the subpage...</p>\n\
      <p><b>Graphik:</b> Ah, I believe there is a misunderstanding. The discussion\
      \ is being on this page is an excellent idea, as I said.</p>\n<p>I was suggesting\
      \ that the Chongq My Links tool not be located on the same page as the spammed\
      \ links repository. That way, the tool could automatically add the links to\
      \ the Chongqing Page without the user ever loading it.</p>\n<p><b>Mychaeel:</b>\
      \ Auto-adding links to the <a href=\"/Legacy:Chongqing_Page\" title=\"Legacy:Chongqing\
      \ Page\">Chongqing Page</a> through the Chongq My Links! tool isn't in yet (not\
      \ as trivial as it may sound), but I've added functionality to sort and remove\
      \ duplicates from the list: Just copy the existing chongqing links into the\
      \ second textbox and press \"Sort and Remove Duplicates\".</p>\n<hr />\n<p><b>Mychaeel:</b>\
      \ The most recent spammer added the following (badly transcribed) Russian comment\
      \ amidst his spam links: \"Ne udaliyt – derju pod kontrolem\" (followed by a\
      \ great many exclamation marks). My father, who knows Russian, tells me that\
      \ this can be translated as \"Don't remove – I'm keeping it under control!\"\
      \ &#160;:D</p>\n<p><b>captaink:</b> If you Google the quote, it leads to a Russian\
      \ SEO forum. o_0</p>\n<p><b>Zxanphorian:</b> lol</p>\n<hr />\n<p><b>xX)(Xx:</b>\
      \ When i try to edit a page ( <a href=\"/Legacy:ZoneInfo_(UT)\" title=\"Legacy:ZoneInfo\
      \ (UT)\">ZoneInfo (UT)</a> i get sent to the chongqing page, even though im\
      \ not spamming, i even get sent here when i try to preview what ive done, i\
      \ think it might be because there is already an off-wiki link there? Or perhaps\
      \ it doesnt like my IP hehe, well anyway, im not trying to spam, is it possible\
      \ to find out why this is happening?</p>\n<p><b>Tarquin:</b> The problem is\
      \ the phrase 'bGravity...Zone' (without the dots). We set the script up to block\
      \ a guy who kept writing 'GraViTy...Z'. I'll fix it tomorrow.</p>\n<p><b>xX)(Xx:</b>\
      \ Thanks Tarquin&#160;:)</p>\n<p>Damn spammers&#160;:(</p>\n<p><b>Wormbo:</b>\
      \ The guy was here again. This time he spammed \"GRav.IT.Z\" and various dots\
      \ on several pages and from several IPs.</p>\n<p>BTW: The Chongq My Links tool\
      \ sorts case-insensitive, but removes duplicates case-sensitively based on that\
      \ sorting. The result is that we get the same key words repeating, e.g. \"sports\
      \ betting\", \"Sports Betting\", \"sports betting\", \"Sports Betting\" and\
      \ so on.</p>\n<hr />\n<p><b>Mosquito:</b> This one has to be the worst yet.</p>\n\
      <p>If it wasn't for the quick acting of the BU admin it might not have stopped\
      \ until the whole wiki was toast.</p>\n<p><b>Foxpaw:</b> Erm.. I noticed some\
      \ pages I ended up reverting to their spammed state. I was reverting in batches\
      \ of 30 or so pages simultaneously so there was some delay between when I checked\
      \ the recent changes list to when I reverted the page.</p>\n<p><b>Mosquito:</b>\
      \ Fair enough. Though, when it came to the page the speed in which the whole\
      \ spamming was happening was incredible. It would revert one page and then 3\
      \ more would be spammed. Coincidentally enough I had the platoon theme playing\
      \ on my playlist when I loaded the recent pages for the first time</p>\n<p><b>Tarquin:</b>\
      \ I expect the spammer was running a bot.&#160;:(</p>\n<p><b>EntropicLqd:</b>\
      \ Couldn't we update the Edit/Save page functionality to randomly generate the\
      \ name of the submission form and form elements and store them on the user's\
      \ session. Then the update script could compare the fields received with the\
      \ names held on the session and if they don't match then the update is not performed.\
      \ If the labels on the buttons were either images (with generated names) or\
      \ selected from a larger pool of possible labels (e.g. Save, Update, Store Changes,\
      \ you get the idea) then you'd effectively disable a bot as it would have nothing\
      \ to work with. Hopefully I've explained what I mean.</p>\n<p><b>El Muerte:</b>\
      \ Here's an implementation idea that would stop bots. Add an hidden field that\
      \ contains a hash of the page name and some other stuff. When an edit is submitted\
      \ the hash is checked to see if it's correct. This way you always have to visit\
      \ the edit page and can't use an automated POST script. e.g. hash = md5(pageTitle+userAddr+secretKey)\
      \ . When the submission doesn't have the correct hash the edit page is popped\
      \ up again with your edited text. This way when you get a new IP assigned in\
      \ the meanwhile your edit won't be lost.</p>\n<p><b>Mychaeel:</b> That's a pretty\
      \ smart idea, I think. Right now most of the spam seems to be blocked by the\
      \ link filter, but I'll look into implementing the hash idea as well.</p>\n\
      <hr />\n<p><b>Mychaeel:</b> I've added code that automatically puts people who\
      \ add too many links to a page on read-only (with a \"low\" threshold which\
      \ only causes a temporary ban and a \"high\" one that causes an immediate permanent\
      \ ban). It also dispatches an email to the wiki admin in those cases to ensure\
      \ they can react in a timely manner if necessary.</p>\n<p><b>Foxpaw:</b> How\
      \ many links is \"too many?\" It seems like a person who was refactoring a page\
      \ or merging two pages might add a number of links in a single edit and trip\
      \ the ban.</p>\n<p><b>Mychaeel:</b> I've currently defined 15 new external links\
      \ as \"too many.\" (Between 5 and 14 new external links, the user is temporarily\
      \ put on read-only; no harm done by that. Just sit it out or wait until an admin\
      \ undoes the auto-ban before it times out.) I doubt many pages even contain\
      \ that number of external links.</p>\n<p><b>Mychaeel:</b> ...and already caught\
      \ one tonight who tried to add 451 links to <a href=\"/Legacy:Brush_Preservation\"\
      \ title=\"Legacy:Brush Preservation\">Brush Preservation</a>... &#160;:D</p>\n\
      <p><b>Tarquin:</b> I don't think pages have that many external links. At most\
      \ I usually see maybe up to 5 in the External Links section. Most links on wiki\
      \ pages are internal. Good work Mych!&#160;:D</p>\n<p><b>strider:</b> Brilliant\
      \ idea Mychaeel!</p>\n<p><b>Mychaeel:</b> This afternoon, the spam trap caught\
      \ a spammer who added the modest amount of 12 links to <a href=\"/Legacy:Using_The_Wiki\"\
      \ title=\"Legacy:Using The Wiki\">Using the Wiki</a> and thus got temporarily\
      \ banned at first. The automatic notification gave me the opportunity to make\
      \ the ban permanent and revert the changes within minutes.</p>\n<p><b>Mychaeel:</b>\
      \ By the way, I'm getting down to business about the \"file an official complaint\"\
      \ part. I've done so for the last two spammers, and we'll see what the response\
      \ is.</p>\n<p><b>Wormbo:</b> I can't say which of these two good news I like\
      \ better.&#160;:D</p>\n<p><b>strider:</b> You can now add rel=nofollow to anchor\
      \ tags to make it so external links aren't counted in search engine stats. Wikipedia\
      \ have already added this feature, and doing this might help kill spam.</p>\n\
      <p><b>Mychaeel:</b> Spammers don't spam because spam is effective; it's sufficient\
      \ that somebody <i>tells them</i> it is. (That's especially evident in those\
      \ spam attacks on our wiki with code that didn't even render as links...) Reducing\
      \ the effectivity of spam won't help, because the effectivity of spam is a non-issue\
      \ to spammers. They <i>believe</i> it's effective (or are made to by people\
      \ who sell them spamming services and software), and that's enough. Spam has\
      \ become a self-sufficient phenomenon.</p>\n<p>That said, the spam filter just\
      \ caught another one adding 536 links to \"Inside The Death Chamber - Exploring\
      \ Executions\" before any damage could be done. Funnily enough, we don't even\
      \ have that page.</p>\n<dl>\n<dd><b>Wormbo:</b> I've removed the wiki link markup\
      \ to \"Inside The Death Chamber - Exploring Executions\" from your comment.\
      \ Someone created that page only with the word \"You\" on it. After I deleted\
      \ it, it was created again with the same content.</dd>\n</dl>\n<p><b>Mychaeel:</b>\
      \ Caught three more in the course of the afternoon before they could do damage.</p>\n\
      <p><b>Mosquito:</b> Have you considered passing this around to other wikis,\
      \ I'd imagine some are getting hit pretty hard these days.</p>\n<p><b>Mychaeel:</b>\
      \ Actually, I pinched the basic idea for this filter from <a href=\"http://www.usemod.com/cgi-bin/mb.pl?ShotgunSpam\"\
      \ class=\"extiw\" title=\"meatball:ShotgunSpam\">MeatBall:ShotgunSpam</a>, so\
      \ other wikis are probably already aware of this idea. Our implementation just\
      \ adds auto-banning and email notifications.</p>\n<p><b>Mychaeel:</b> One of\
      \ the ISPs I sent a complaint to has sent a reply:</p>\n<blockquote class=\"\
      legacyquote\">\n<p>Dear Sir/Madam,</p>\n<p><br /></p>\n<p>We have already issued\
      \ a warning to the user to ensure that such activity is not repeated in future.\
      \ Hence we would request you to consider this case as closed. The Trouble Ticket\
      \ Number for this complaint is '2973'.</p>\n<p><br /></p>\n<p>Please do contact\
      \ us if the incident repeats again.</p>\n<p><br /></p>\n<p>Assuring you the\
      \ best of our services.</p>\n<p><br /></p>\n<p>Thanking you.</p>\n<p><br /></p>\n\
      <p>Yours Sincerely,</p>\n<p><br /></p>\n<p>Antiabuse Support</p>\n</blockquote>\n\
      <p>E-mail&#160;: Antiabuse.Support@relianceinfo.com</p>\n<p>Phone&#160;: 91-(022)\
      \ -30388464</p>\n<p><b>Mychaeel:</b> ...and one more:</p>\n<blockquote class=\"\
      legacyquote\">\n<p>Hi,</p>\n<p><br /></p>\n<p>we will take the necessary actions\
      \ to stop this kind of illegal activity.</p>\n<p><br /></p>\n<p>Ystävällisin\
      \ terveisin</p>\n</blockquote>\n<p>Best Regards</p>\n<p>EUnet Finland</p>\n\
      <p>puh +358 9 4243 3205</p>\n<p>fax +358 9 4243 0601</p>\n<p>Linnoitustie 4\
      \ B (Alto)</p>\n<p>02600 Espoo</p>\n<hr />\n<p><b>EntropicLqd:</b> Is the <a\
      \ href=\"/Legacy:Recent_Changes\" title=\"Legacy:Recent Changes\" class=\"mw-redirect\"\
      >Recent Changes</a> page indexed? It occured to me that the recent page reverts\
      \ were not so much an attempt at vandalising the Wiki page but merely a way\
      \ of getting a single link listed many times by placing the target link in the\
      \ change summary. Might be worth preventing URLs from being placed in the change\
      \ description.</p>\n<p><b>Mychaeel:</b> Pages whose URLs look like <code><a\
      \ rel=\"nofollow\" class=\"external free\" href=\"http://wiki.beyondunreal.com/wiki\"\
      >http://wiki.beyondunreal.com/wiki</a>?...</code> are exempt from indexing,\
      \ and after reversal, the spammy summary is removed from <a href=\"/Legacy:Recent_Changes\"\
      \ title=\"Legacy:Recent Changes\" class=\"mw-redirect\">Recent Changes</a> too.\
      \ Of course, spammers wouldn't even care about that subtlety if we put it on\
      \ a flashing banner across the editing page.</p>\n<p>Perhaps you're right with\
      \ that idea, though I'm unclear about why someone would want a plain-text URL\
      \ spammed somewhere; it's not like any search engine would consider that a \"\
      link\" to increase the URL target's PageRank.</p>\n<p>I could add a bit of code\
      \ that prevents a page from being saved (and redirects the user elsewhere to\
      \ an explanation) when an URL is placed in the summary field (and put a hint\
      \ not to do that in the caption text below).</p>\n<p>By the same token, I'll\
      \ probably extend the link counting algorithm to include prior page revisions\
      \ created by the same user as well. I had anticipated this development for a\
      \ while – it's a well-established fact that spammers' motivation to spam is\
      \ not impaired by even the most coarse hints that it's ultimately futile even\
      \ if it manages to get past any technical measures, as demonstrated by waves\
      \ of spam emails crafted to bypass spam filters. I just hadn't been able to\
      \ get myself to actually implement that.</p>\n<p>...which reminds me of shooting\
      \ off two complaints about those recent spammers.</p>\n<p><b>Birelli:</b> I\
      \ think the last two are actually the same spammer. Two spams to a *.*.su both\
      \ for gambling (presumably) spamming in an unusual way, it's probably the same\
      \ one. And given that one IP is on the East Coast US, the other in Australia,\
      \ it's a fair bet that it's a redirect.</p>\n<hr />\n<p><b>Joe@Chongqed:</b>\
      \ Hi. I saw your Temporary_Read-Only page. I like the idea. I added it to our\
      \ page discussing <a rel=\"nofollow\" class=\"external text\" href=\"http://wiki.chongqed.org//WikiSpam\"\
      >spam protection methods</a>. I am calling it AutoBan for now, do you have any\
      \ better name? You mentioned above the idea comes from ShotGunSpam. Is this\
      \ method unique enough to deserve a section of its own or should I just add\
      \ to our description of ShotGunSpam?</p>\n<hr />\n<p><b>MythOpus:</b> Awhile\
      \ ago I caught a spammer who edited the actual hyper link of links that were\
      \ already on a few pages. I don't think the auto ban incorporates that so is\
      \ there any way we can shut those spamming possibilities down? Also, it may\
      \ be a little over the top but, a captcha system might be a good idea. Or, we\
      \ can fake one&#160;:) Make a fake little picture, put it on the editing pages\
      \ and spam bots would have to type in the special number. If someone get's a\
      \ spam bot in we can simply change the pic to something else and change the\
      \ pass?</p>\n<p><b>zugy:</b> Hmmm-I wonder...It can't be too hard to generate\
      \ a security code thing, would it? You see them everywhere, these days....</p>\n\
      <p><b>Draconx:</b> Guah! That temp ban thing raped me&#160;:( I revert the <a\
      \ href=\"/Legacy:Unreal_Engine_Versions\" title=\"Legacy:Unreal Engine Versions\"\
      >Unreal Engine Versions</a> page which has <b>twenty-six</b> external links\
      \ on it and get owned for 4h 20m&#160;:(</p>\n<p><b>Mychaeel:</b> I had undone\
      \ your temporary ban within minutes of getting the notification email. Thanks\
      \ for the trouble. &#160;:-)</p>\n<p><b>zugy:</b> Bummer-I was gonna revert\
      \ that myself except- 1) didn't know how and 2) thought it might be an admin-only\
      \ kind of thing...</p>\n<hr />\n<p><b>Guest:</b> Mych, have you considered making\
      \ this page auto-update when a ban or other relevant event took place? I know\
      \ a feature like this wouldn't code itself, but it seems like you spend a lot\
      \ of time updating the table.</p>\n<p><b>MythOpus:</b> Mych, are you have the\
      \ time of your life or what?</p>\n<p><b>Mychaeel:</b> Actually, manually keeping\
      \ track of the auto-bans gets tiring after the first hundred or so. I'd rather\
      \ see spammers discouraged or (gasp!) on their way of understanding that what\
      \ they're doing is wrong on so many levels.</p>\n<p><b>MythOpus:</b> I'll be\
      \ waiting for the day when that happens. Sadly, most spammers these days are\
      \ getting paid big bucks to do what they do. It's a shame. You should consider\
      \ the auto-update idea though. I don't think it would be too hard to manage?</p>\n\
      <p><b>Mychaeel:</b> Mostly I want to confirm that it's really a spamming attempt\
      \ and not a false positive before I log anything anywhere.</p>\n<p><b>T1:</b>\
      \ I have an idea that may work to help slow down people from writing bots that\
      \ change large amounts of pages with only single links, therefore avoiding the\
      \ autoban. You could prevent non-registered users from doing many edits at once,\
      \ by only allowing 1 edit every 10 seconds. Basically you would store a global\
      \ \"last edit time\" and every time non-registered-user makes an edit, check\
      \ if there has been an edit in the last 10 seconds by a non-registered-user\
      \ (it doesn't matter if it's the same or a different one). If so, then you'd\
      \ get a new page that said, please wait X seconds, but it still has the textbox\
      \ with what you wrote so you don't lose your submission. This would prevent\
      \ that situation some referred to farther up the page where there was bot that\
      \ was spamming faster than wikizens could revert. The non-registered bot would\
      \ be slower, but it wouldn't affect non-registered users who aren't spamming\
      \ very much. You might say that the bot would then simply register before spamming,\
      \ but you could put a ten second limit on registering also, therefore each time\
      \ it made an edit, it would still be slower because the autoban or a good wikizen\
      \ would probably catch it. The only problem I forsee is people that don't register\
      \ and want to help against a bot, they may not be able to do much except slow\
      \ it down if they get a lucky shot into the minimal gap between the end of the\
      \ ten seconds and the next bot attack.</p>\n<p>Also, if the ENTIRE text of the\
      \ page is deleted and replaced with more than two or three links and no non-links,\
      \ isn't that an obvious spam attempt that should be auto-detected, even if it's\
      \ less links than the standard autoban link limit?</p>\n<p><b>craze:</b> i think\
      \ the Wiki just got a large dose of chongqing, tryed to see if i got my help\
      \ desk question answered, and woulkda ya look at tehh, massive page edits, all\
      \ with ad links in em'....</p>\n<p><b>T1:</b> Another bot prevention method:\
      \ If some one adds the same text to more than two pages, autotempban. Happens\
      \ a 3rd time? it's an autopermaban. Now, if only someone would implement these\
      \ things, it'd slow dow/inconvience/prevent some spammers. Also, the statistics\
      \ shouldn't be on this page because then probably no one notices actual discussion\
      \ on this page because they think it's the stats being updated..</p>\n<p><b>El\
      \ Muerte:</b> I think the spam check needs a fix to also scan for bad defined\
      \ links. In this case the bot should have gotten an tempban because he submitted\
      \ 6 links, but the links were not correct unrealwiki markup.</p>\n<p><b>Tarquin:</b>\
      \ I'm locking the wiki for a bit.</p>\n<p><b>Wormbo:</b> Hmm, all those changes\
      \ came from different IPs. A coordinated distributed spam attack? I think I\
      \ smell a botnet.</p>\n<p><b>Tarquin:</b> I'm looking at our spam detection\
      \ code, and Mych's use of regexps is beyond me&#160;:(</p>\n<p><b>Mychaeel:</b>\
      \ The spam trap <i>did</i> spring – once for each of the 41 different IP addresses\
      \ the spammer used.</p>\n<p>Currently, the spam trap is very simple. Yes, we\
      \ can make it more sophisticated, but in the end it'll always be just a small\
      \ Perl script, while spammers will always possess some level of human intelligence\
      \ (even though the fact that this small Perl script of ours manages to outsmart\
      \ about 99% of them speaks volumes about it). So, while we can put huge amounts\
      \ of brains into creating a script which catches an even higher percentage of\
      \ spammers, we'll still never reach the point where we can be sure it'll catch\
      \ <i>all</i> spammers. I don't think we can currently get any better effort/success\
      \ ratio than we have already.</p>\n<p>Several trusted wikizens here have been\
      \ given a secret URL to lock the wiki in case of emergency. Why did nobody use\
      \ it?</p>\n<p><b>T1:</b> I know perl myself, and I'd love to lend a hand in\
      \ improving the script. In my opinion there is nothing that cannot be improved\
      \ upon, and I find programming much more fun than wikignoming. 99% is still\
      \ not 100%. Besides, my hard drive on my main pc is messed up, and I have absolutely\
      \ NOTHING to do right now, so I'd absolutely love to do some semi-constructive\
      \ programming.</p>\n<p>Also, I'm checking for other wikis that got hit by the\
      \ same spammer, and I've found these so far:</p>\n<p><a rel=\"nofollow\" class=\"\
      external free\" href=\"http://www.intertwingly.net/wiki/pie/FrontPage\">http://www.intertwingly.net/wiki/pie/FrontPage</a></p>\n\
      <p><a rel=\"nofollow\" class=\"external free\" href=\"http://wiki.python.org/moin/AtAudioSprint?action=diff\"\
      >http://wiki.python.org/moin/AtAudioSprint?action=diff</a></p>\n<p><a rel=\"\
      nofollow\" class=\"external free\" href=\"http://wiki.43folders.com/index.php/Special:Recentchanges\"\
      >http://wiki.43folders.com/index.php/Special:Recentchanges</a></p>\n<p><a rel=\"\
      nofollow\" class=\"external free\" href=\"http://wiki.kde.org/tiki-index.php?PHPSESSID=4942f9c1fedf2525cddb6c6e878ea769\"\
      >http://wiki.kde.org/tiki-index.php?PHPSESSID=4942f9c1fedf2525cddb6c6e878ea769</a></p>\n\
      <p><b>Tarquin:</b> Mych, my thought was actually to make the trap <i>simpler</i>\
      \ – to check purely for URLs, rather than URL links. But if it caught the spammer...\
      \ cool&#160;:) BTW, what does '&amp;#' do in the regexp? (line 4388)</p>\n<p><b>T1:</b>\
      \ '&amp;#'????? That's not even in the man page.</p>\n<dl>\n<dd><b>Mychaeel:</b>\
      \ It looks for the string \"&amp;#\"&#160;;-) – those pattern matches are part\
      \ of the \"old\" spam detection code which was added when wikis were flooded\
      \ by \"chinese spam.\" The auto-banning URL filter is somewhere else and indeed\
      \ simply looks for occurrences of <code>/UTF8_REPLACEMENThttp:/i</code>.</dd>\n\
      </dl>\n<p><b>craze:</b> although i know very little on the subject, how about\
      \ having the spam detection script(when tripped) begin to traceroute all edits\
      \ within the next x secconds/x minuites and check for matching IPs near the\
      \ end of the list, just to make sure its not just one person with a network\
      \ of IPs attacking teh Wiki...(this probably makes no sence...)</p>\n<p><b>El\
      \ Muerte:</b> how about using DNSBL to check the host? The wiki doesn't have\
      \ that many edits so the overhead might be low. Although in the last case it\
      \ wouldn't have stopped the sammer since he was using a zombie network.</p>\n\
      <p><b>Switch`:</b> How about adding user registration and blocking anonymous\
      \ posting? Maybe not exactly in spirit of wiki but this is \"The Unreal Engine\
      \ Documentation Site\" for developers by developers first, wiki second.</p>\n\
      <p><b>craze:</b> ahh thats thwe word i was thinking of, zombie, just check for\
      \ a common IP twards the end after it starts going through ISPs, just check\
      \ if someone after the ISPs IP has something after it, if i got this right,\
      \ IPs twards the end of the list would start to match(xept the last one) in\
      \ the trace route, then if it starts matching abd the anti spam scripts has\
      \ been tripped just discard the edit</p>\n<hr />\n<p><b>MythOpus:</b> Another\
      \ spam attempt just occured. The spammer mass joined and logged in with several\
      \ usernames and link spammed some pages. Apparently, he thought he could trick\
      \ us by deleting some of the links on the pages as to make us think he deleted\
      \ all of the links. When will the maturity level of these 'geniuses' level off\
      \ with normal standards?</p>\n<p><b>Wormbo:</b> Did we already discuss these\
      \ number/letter combinations presented through an image and the user has to\
      \ type it in? We could add something like this to the edit page for each IP\
      \ that didn't properly respond to an image yet. After this verification the\
      \ user or IP won't have to do it again. If verification fails the user will\
      \ only see the preview page. (so the edits aren't lost)</p>\n<p><b>T1:</b> Same\
      \ spammer as last time.</p>\n<p><a rel=\"nofollow\" class=\"external free\"\
      \ href=\"http://wiki.kde.org/\">http://wiki.kde.org/</a></p>\n<p>They had such\
      \ an image verification system, spammers hit them just as hard as us last time,\
      \ if not harder.</p>\n<hr />\n<p><b>Wormbo:</b> We need a way to revert pages\
      \ with many links. <a href=\"/Legacy:Wiki_Integration/Browser_Sidebar\" title=\"\
      Legacy:Wiki Integration/Browser Sidebar\">Wiki Integration/Browser Sidebar</a>\
      \ got overridden with spam and attempts to revert it result in permanent read-only,\
      \ without any changes to the page.</p>\n<p><b>Mychaeel:</b> Short-term solutions:\
      \ Admins are now exempt from the link threshold (should have been that way from\
      \ the start). Long-term solution: Reverting a page to a previous version should\
      \ be allowed by anyone no matter what it does to the number of links.</p>\n\
      <hr />\n<p><b>Mychaeel:</b> That <code>display:</code> <code>none</code>-type\
      \ spam is blocked now as well.</p>\n<hr />\n<p><b>Tarquin:</b> A lot of the\
      \ links here go to 404s on the Chongqing site.</p>\n<p><b>Kartoshka:</b> 404s\
      \ because whoever added the links to the Chongqing page didn't submit them to\
      \ chongqed.org?</p>\n\n<!-- \nNewPP limit report\nCPU time usage: 0.119 seconds\n\
      Real time usage: 0.201 seconds\nPreprocessor visited node count: 33/1000000\n\
      Preprocessor generated node count: 113/1000000\nPost‐expand include size: 1276/2097152\
      \ bytes\nTemplate argument size: 522/2097152 bytes\nHighest expansion depth:\
      \ 3/40\nExpensive parser function count: 0/100\n-->\n\n<!-- \nTransclusion expansion\
      \ time report (%,ms,calls,template)\n100.00%    5.584      1 - -total\n 93.54%\
      \    5.223      2 - Template:Innerbox\n-->\n\n<!-- Saved in parser cache with\
      \ key wiki:pcache:idhash:887-0!*!0!*!*!*!* and timestamp 20221118130836 and\
      \ revision id 2454\n -->\n"
  categories: []
  links:
  - ns: 100
    exists: true
    name: "Legacy:Mychaeel"
  - ns: 100
    exists: true
    name: "Legacy:Wiki Integration/Browser Sidebar"
  - ns: 100
    exists: true
    name: "Legacy:Chongqing Page"
  - ns: 100
    exists: true
    name: "Legacy:Recent Changes"
  - ns: 0
    exists: false
    name: "Localcgi:chongqme"
  - ns: 100
    exists: true
    name: "Legacy:Chongqing Page/Permanent Read-Only"
  - ns: 100
    exists: true
    name: "Legacy:Using The Wiki"
  - ns: 100
    exists: true
    name: "Legacy:Wiki Admin"
  - ns: 100
    exists: true
    name: "Legacy:ZoneInfo (UT)"
  - ns: 100
    exists: true
    name: "Legacy:Brush Preservation"
  - ns: 100
    exists: true
    name: "Legacy:Chongqing Page/Temporary Read-Only"
  - ns: 100
    exists: true
    name: "Legacy:Unreal Engine Versions"
  - ns: 100
    exists: true
    name: "Legacy:Chongqing Page/Statistics"
  templates:
  - ns: 10
    exists: true
    name: "Template:Innerbox"
  images: []
  externallinks:
  - "http://wiki.beyondunreal.com/robots.txt"
  - "http://www.chongqed.org"
  - "http://chongqed.org"
  - "http://wiki.beyondunreal.com/wiki"
  - "http://wiki.kde.org/"
  - "http://wiki.chongqed.org"
  - "http://wiki.python.org/moin/AtAudioSprint?action=diff"
  - "http://www.intertwingly.net/wiki/pie/FrontPage"
  - "http://spammers.chongqed.org/business%20transcription"
  - "http://wiki.chongqed.org//WikiSpam"
  - "http://wiki.43folders.com/index.php/Special:Recentchanges"
  - "http://spammers.chongqed.org/Effexor%20and%20weight%20loss"
  - "http://wiki.kde.org/tiki-index.php?PHPSESSID=4942f9c1fedf2525cddb6c6e878ea769"
  sections:
  - toclevel: 1
    level: "2"
    line: "Discussion"
    number: "1"
    index: "1"
    fromtitle: "Legacy:Chongqing_Page/Discuss"
    byteoffset: 1390
    anchor: "Discussion"
  displaytitle: "Legacy:Chongqing Page/Discuss"
  iwlinks:
  - prefix: "meatball"
    url: "http://www.usemod.com/cgi-bin/mb.pl?ShotgunSpam"
    name: "meatball:ShotgunSpam"
  wikitext:
    text: "There are two types of \"bans\" (actually, read-only states) imposed by\
      \ the automatic spam filter:\n\n* '''[[:{{SUBJECTSPACE}}:{{BASEPAGENAME}}/Temporary\
      \ Read-Only|Temporary Bans]]''' are imposed for adding several, but not too\
      \ many links to a page.  The page (with the links) is saved, and the user is\
      \ put on read-only for ten minutes for each link he or she added.\n\n* '''[[:{{SUBJECTSPACE}}:{{BASEPAGENAME}}/Permanent\
      \ Read-Only|Permanent Bans]]''' are imposed for adding too many links to a page.\
      \  The changes to the page are discarded, and the user is put on read-only permanently.\n\
      \nI'm intentionally keeping the exact number of links required to trigger either\
      \ of those cases vague.  I'd rather not have spammers fine-tune their spamming\
      \ attempts using that information.  ;-)\n\nIn both cases, an email containing\
      \ time, IP address, modified page name and submitted page content is dispatched\
      \ to the [[Legacy:Wiki Admin|wiki admin]]s.  This allows them to react quickly\
      \ to temporary bans (to revert the changes and make the ban permanent if a spammer\
      \ was caught, or to undo the automatic ban on false positives), and it serves\
      \ as proof when filing complaints to the spammers' ISPs.\n\nIf somebody would\
      \ like to be added to the list of users receiving those notification emails,\
      \ drop [[Legacy:Mychaeel|Mychaeel]] a line.\n\nSee also [[Legacy:Chongqing Page/Statistics|Chongqing\
      \ Page/Statistics]].\n\n==Discussion==\n\n'''Tarquin:''' Just for added fun,\
      \ I have hacked our page database so even the old revision of the page this\
      \ guy edited leads to chongqed.org  bwahahahaha :D\n\n'''RavuAlHemio:''' :D\
      \ You're kicking the spammers anywhere you can, huh? :D\n\n'''anonymous:'''\
      \ Yes indeed folks go on treating them *******s the hard way. I had real troubles\
      \ with spammers myself glad to see some action\n\n'''Foxpaw:''' The ones I just\
      \ posted didn't really have any keywords, not ones legible on my PC, anyway,\
      \ so I just replaced the keywords with the URL of the sites..\n\n'''MythOpus:'''\
      \ This spamming is making me mad.  Grr.  I suggest we keep it open to the public\
      \ to edit as they see fit, but I think we should have people sign up for the\
      \ privelage.  We could really make use of the passwords in the preferences menu\
      \ for this and it will help with the spam problem and if it continues then we\
      \ could easilly find out EXACTLY who is doing it.  Of course, that may go against\
      \ what this wiki was meant for but still...  Desperate Times, Desperate Measures.\n\
      \n'''Graphik:''' Hello Myth. :)\n\nTimes aren't desperate, just annoying. ;)\
      \ I thought of the same thing, but in the end it is like you said, that's not\
      \ what a wiki is meant for. Eventually they'll give up; it's very unsatisfying\
      \ to see your work undone again and again. We already can find out who it is\
      \ without registration, and I just banned one person today. :)\n\n'''MythOpus:'''\
      \ Note to all Spammers... Bow now, or bow later... muhahahahaphmuhahaMPH!!\n\
      \n'''Mychaeel:''' I don't really believe in technical means to stop spammers,\
      \ but since those asian spam links are easy enough to spot and never used anywhere\
      \ else on the Wiki, I've added a bit of code to the script that redirects people\
      \ trying to save a page which contains such a link.  Try it yourself...  :-)\n\
      \n'''Foxpaw:''' The spamming has been intense lately. Is there a known reason\
      \ for the surge in spamming? I don't remember it ever happening less than a\
      \ year ago, not it seems within the last week or so it's been several spamvertizements\
      \ a day.\n\n'''Mychaeel:''' I think the one and only reason is that spammers\
      \ have discovered wikis.  Now, true to their \"I'm the only thing that matters\
      \ in the world\" attitude, they hook onto wikis everywhere in order to milk\
      \ them for whatever benefit they see in doing so until they're dead and useless\
      \ &ndash; like a deadly virus.  Fortunately for us, their limited view of the\
      \ world doesn't quite match reality, so there's still hope for the social rest\
      \ of the world.\n\n'''Tarquin:''' Does Google's bot see old revisions of pages?\n\
      \n'''Mychaeel:''' Our [http://wiki.beyondunreal.com/robots.txt robots.txt] wasn't\
      \ properly set up to prevent this when I checked (only disallowing access to\
      \ <code>/cgi-bin/wiki.pl?action=</code>, not <code>/wiki?action=</code>).  I've\
      \ fixed this.\n\n'''OverloadUT:''' I noticed that if you check the \"minor edit\"\
      \ box when editing a page, it does not jump to the top of the list on the [[Legacy:Recent\
      \ Changes|Recent Changes]] page. If a spammer figured this out, couldn't they\
      \ check that box on all their spammed pages, and it would take much longer to\
      \ find them?  Or is there some sort of protection against this?\n\n'''Tarquin:'''\
      \ Yes: set your preferences to see minor edits! :)\n\n'''Mychaeel:''' The [[localcgi:chongqme|chongqing\
      \ tool]] I added today makes is pretty simple to chongqify stuff added by a\
      \ spammer; however, it doesn't register those keywords at http://chongqed.org.\
      \  The chongqing links still do their purpose, but they lead to an unimpressive\
      \ \"The keyword [...] does not seem to be in the database\" page.\n\n'''Foxpaw:'''\
      \ Just thought of something - this page is going to get huge, and sufficiently\
      \ huge pages seem to time out when you try to save them - that could become\
      \ a problem as this page grows. The chongquing tool was very handy though, and\
      \ just in time, as this spamming binge was enormous.\n\n'''Parallax:''' Pagerank\
      \ on google is based on the number of links into the linking page. We should\
      \ have every single page in the wiki containing a link to this Chongqing page\
      \ so that it gets a very very high page rank. Such a link to this page could\
      \ be very discreet. That would mean that this page would have more authority\
      \ when it comes to destroying the page rank of the spammers.\n\n'''Tarquin:'''\
      \ Yup. We could add it to the footer (in a small font) or something.\n\n'''Foxpaw:'''\
      \ Not only would that greatly increase loading times for pages on the wiki,\
      \ but I don't think that it would work. Search engine spiders probrably only\
      \ recognize a given link/keyword combination once per domain - otherwise spammers\
      \ wouldn't have to spam wikis and blogs, they could just spam one domain (that\
      \ they controlled) a whole ton of times.\n\n'''Mychaeel:''' I don't think anybody's\
      \ talking about adding the links ''on'' this page to the footer &ndash; it's\
      \ about adding a link ''to'' this page to the footer.  That really wouldn't\
      \ increase loading times a lot.\n\n'''Wormbo:''' I've just removed over 50%\
      \ of the links because they were duplicates. Please check that first if you\
      \ add them so we can keep the page size within reasonable limits. Does linking\
      \ with an URL as the link text actually help?\n\n'''Halz''' Yeah those URL links\
      \ under 'Miscellaneous' are less effective chongqing links, particularly as\
      \ they all point to the same page, rather than specific parts of the chongqed.org\
      \ database. Also the links like [http://spammers.chongqed.org/Effexor%20and%20weight%20loss\
      \ Effexor and weight loss] are keywords which you haven't told chongqed.org\
      \ about. To chongq  effectively, you have to sumbit the spammer to chongqed.org,\
      \ wait for the keywords to be accepted into the database, and then link using\
      \ the keywords. So a link like [http://spammers.chongqed.org/business%20transcription\
      \ Business Transcription] is best. However any links to [http://www.chongqed.org\
      \ chongqed.org] will help boost its general ranking. Also as Foxpaw was saying,\
      \ links from various different domains is better than lots of links from just\
      \ one page, so if you folks have your own homepage, put a link there too!\n\n\
      '''Savannahlion:''' Yep, I've been Chongqed. Initially, this <insert appropriate\
      \ insult here> started by dumping a few links in the Sandbox. Simple enough,\
      \ I simply banned him. Problem solved for a few months.\n\nThen he returns,\
      \ and creates two pages in his interest. Deleted them. A few days later he assaults\
      \ about eight or so pages with piles of his links. I call him Mr. Seo, as a\
      \ hint. Unfortunately, the tactic of simply Chngqing his links won't work too\
      \ well. He's creating wiki links within existing text without regard for context.\
      \ Links within code where the [ and ] would be misinterpreted. A spam link within\
      \ the words Counter-Strike, which is woefully outside of the context. Even a\
      \ link to his spam within a reference to Google. Completely random words. \n\
      \nI created a patch to prevent his lame additions. This was before I realized\
      \ Tarquin solved the problem but after 4 hours of site clean up, coding, patching,\
      \ and testing and 16 hours at work.\n\nI'm going to look intto creating a Chongq\
      \ page to support all of this. But the details of implemenation are kind of\
      \ hazy. I'm going to have to mull over implementation details after work. Thank\
      \ you Tarquin for pointing me in the right direction. But I would still like\
      \ to pick your brain personally. IRC, ICQ, something.\n\n'''Tarquin:''' A Chongq\
      \ page simply links to the Chongq site to damage the spammer's google rating.\
      \ I'm not in a position at the moment to be often on IRC, so you're best off\
      \ reading up at the Chongq site to see how it works, or catching another wiki\
      \ admin on our channel. Good luck dealing with this guy. Does his IP change\
      \ a lot? You can always try banning him. Or you can hack to wiki code to refuse\
      \ to save edits containing certain words or URLs.\n\n----\n\n'''Zxanphorian:'''\
      \ OMG to all of those sites!\n\n'''Tarquin:''' I'm thinking we should perhaps\
      \ post on BuF to ask peopel to help the wiki fight spammers by putting a link\
      \ to this page on their websites. What do you all think?\n\n'''Foxpaw:''' Hrmm.\
      \ That would boost the pagerank of this page, but not of Chongqed.org. If possible,\
      \ copying the text from this page to a google-indexed page on their own site\
      \ would probrably work better. That way Chongqed.org gets a higher cumulative\
      \ rank as opposed to those referral links being divided among two sites (here\
      \ and Chongqed.org).\n\nAt least, I don't THINK that being google has a heirarchial\
      \ sort of system where a link to this page would be recognized as an indirect\
      \ link to Chongqed.org.\n\n'''Tarquin:''' We're not trying to raise the rating\
      \ of Chongqed.org, we're trying to connect the keywords the spammers care about\
      \ with the link to Chongqed.org. So yeah, copying this content works too, but\
      \ according to the guys at Chongqed.org, making this page highly-linked to will\
      \ be good too.\n\n'''Foxpaw:''' Err, well, yes, but like I said, I don't think\
      \ that linking here from elsewhere will do anything except raise the pagerank\
      \ of this page, with the keywords used to link here. I wouldn't expect it to\
      \ have any effect on the pagerank of Chongqed.org in combination with the keywords\
      \ here, as I don't think that google counts indirect references like that. \n\
      \nHowever, if they say it will work, I'll take their word for it.\n\n'''ElMuerte:'''\
      \ we need some magic for this page, e.g.:\n* links on this page are read from\
      \ an arbitrary file\n* to add new chongqing links use an addition script, this\
      \ script will:\n** check for duplicates\n** ... do more? ...\n* Clearly mark\
      \ the link to the add script, like: \"If you want to add spam to this site,\
      \ please use the following link\" \n\nThis makes things easier:\n* no duplicates\n\
      * no high loading/saving/editing time for this page\n* easier to catch new spam\
      \ on this page\n\n'''Mychaeel:''' I was thinking about extending the [[localcgi:chongqme|Chongq\
      \ My Links!]] script with duplicate-removal functionality &ndash; I just didn't\
      \ get around to doing it yet.  (Now I've got my computer set up at home again,\
      \ so chances are rising that I'll find the time soon.) &ndash; Anyway, I find\
      \ it amusing that this page is being the main target for spammers these days...\
      \  :p\n\n'''Foxpaw:''' It seems like the logical page to spam, if you think\
      \ about it. Not only do you get the normal \"benefit\" of posting your links,\
      \ but you remove any of your links that may have been chongqed previously too.\n\
      \n'''Mychaeel:''' True... though that explanation implies that spammers were\
      \ that smart, which appears hardly plausible to me (just look at the recent\
      \ spam attempts &ndash; the link tags were so incompetently crafted that they\
      \ didn't even work). &ndash; Maybe this sentence from the edit page header is\
      \ what makes spammers paste their links here: \"Note that old page revisions\
      \ aren't indexed by Google, but the Chongqing Page is.\"\n\n'''Joe@Chongqed:'''\
      \ Hi.  You guys are doing some good chongqing here, thanks.  I just wanted to\
      \ add to what Tarquin said we said about linking (which I had forgot myself).\
      \  By increasing the PageRank for this page it gives the links here more credibility\
      \ and should help their PageRanks too.  Links from a page with higher PageRank\
      \ are more valuable.  As mentioned above, links from your footer or sidebar\
      \ on every page would help the PageRank of this page.  It would also help the\
      \ PageRank of [http://chongqed.org chongqed.org] if you linked from every page,\
      \ but the purpose of chongqed.org is to remove junk from wikis.  While more\
      \ links are great we don't want to clutter up people's wikis so are happy with\
      \ how you guys are doing it.\n\nI also want to invite you guys to our [http://wiki.chongqed.org\
      \ chongqed wiki] in case you missed it, its a newer addition.  We use it to\
      \ discuss chongqing ideas, spammers, antispam protection, etc.  It has been\
      \ up long enough to attract spammers though, it shows how really stupid spammers\
      \ are that they attack chongqed.org, not out of revenge, just regular spamming.\
      \  We have been learning a lot more about spammers recently.  So far all the\
      \ ones that have attacked us have found the pages by searching for other spammer's\
      \ URLs.  They let the other spammers do the dirty work of finding pages where\
      \ their spam will stay long enough for Google to see it.  So by keeping a clean\
      \ wiki you should attract fewer spammers.  Because we list lots of spammer URLs\
      \ (though not links) our page appears to spammers to be a good target.  Although\
      \ we hate spam, we like being spammed on our wiki, it makes chongqing them that\
      \ much easier and more fun.\n\n'''Graphik:''' Thanks for the information on\
      \ your fine service.\n\nI tried to post this once before, but there was an edit\
      \ conflict; this page was spammed. :rolleyes:\n\nI'm not sure if the links I\
      \ reverted should be chongqued from here. They were rather 'adult' in nature\
      \ and might violate  BU's 'no links to porn' hosting condition (In spirit, anyway).\n\
      \n'''Mychaeel:''' Chongqed links point to http://chongqed.org, not porn (unless\
      \ Chongqed.org has changed its agenda since last time I checked).  So just chongq\
      \ ahead!\n\n'''Graphik:''' Done. Damn this page is going to be long after a\
      \ while, so even my cable connection (much more a dial-up user) will start to\
      \ struggle with it. I suggest that we create a subpage to contain the spammed\
      \ links. The Chongq My Links! tool can automagically add them to the subpage\
      \ instead of linking back to here to add them. <insert better-thought-out variant\
      \ of that idea here>\n\n'''Mychaeel:''' Moved the discussion to a subpage.\n\
      \n'''Graphik:''' An excellent idea, but that doesn't solve the problem of having\
      \ to load the parent page to chongq links.\n\n'''Mychaeel:''' Right, but neither\
      \ would keeping the discussion on the main page and putting the links on the\
      \ subpage...\n\n'''Graphik:''' Ah, I believe there is a misunderstanding. The\
      \ discussion is being on this page is an excellent idea, as I said.\n\nI was\
      \ suggesting that the Chongq My Links tool not be located on the same page as\
      \ the spammed links repository. That way, the tool could automatically add the\
      \ links to the Chongqing Page without the user ever loading it.\n\n'''Mychaeel:'''\
      \ Auto-adding links to the [[Legacy:Chongqing Page|Chongqing Page]] through\
      \ the Chongq My Links! tool isn't in yet (not as trivial as it may sound), but\
      \ I've added functionality to sort and remove duplicates from the list: Just\
      \ copy the existing chongqing links into the second textbox and press \"Sort\
      \ and Remove Duplicates\".\n\n----\n\n'''Mychaeel:''' The most recent spammer\
      \ added the following (badly transcribed) Russian comment amidst his spam links:\
      \ \"Ne udaliyt &ndash; derju pod kontrolem\" (followed by a great many exclamation\
      \ marks).  My father, who knows Russian, tells me that this can be translated\
      \ as \"Don't remove &ndash; I'm keeping it under control!\"  :D\n\n'''captaink:'''\
      \ If you Google the quote, it leads to a Russian SEO forum. o_0\n\n'''Zxanphorian:'''\
      \ lol\n\n----\n\n'''xX)(Xx:''' When i try to edit a page ( [[Legacy:ZoneInfo\
      \ (UT)|ZoneInfo (UT)]] i get sent to the chongqing page, even though im not\
      \ spamming, i even get sent here when i try to preview what ive done, i think\
      \ it might be because there is already an off-wiki link there? Or perhaps it\
      \ doesnt like my IP hehe, well anyway, im not trying to spam, is it possible\
      \ to find out why this is happening?\n\n'''Tarquin:''' The problem is the phrase\
      \ 'bGravity...Zone' (without the dots). We set the script up to block a guy\
      \ who kept writing 'GraViTy...Z'. I'll fix it tomorrow.\n\n'''xX)(Xx:''' Thanks\
      \ Tarquin :)\n\nDamn spammers :(\n\n'''Wormbo:''' The guy was here again. This\
      \ time he spammed \"GRav.IT.Z\" and various dots on several pages and from several\
      \ IPs.\n\nBTW: The Chongq My Links tool sorts case-insensitive, but removes\
      \ duplicates case-sensitively based on that sorting. The result is that we get\
      \ the same key words repeating, e.g. \"sports betting\", \"Sports Betting\"\
      , \"sports betting\", \"Sports Betting\" and so on.\n\n----\n\n'''Mosquito:'''\
      \ This one has to be the worst yet.\n\nIf it wasn't for the quick acting of\
      \ the BU admin it might not have stopped until the whole wiki was toast.\n\n\
      '''Foxpaw:''' Erm.. I noticed some pages I ended up reverting to their spammed\
      \ state. I was reverting in batches of 30 or so pages simultaneously so there\
      \ was some delay between when I checked the recent changes list to when I reverted\
      \ the page.\n\n'''Mosquito:''' Fair enough. Though, when it came to the page\
      \ the speed in which the whole spamming was happening was incredible. It would\
      \ revert one page and then 3 more would be spammed. Coincidentally enough I\
      \ had the platoon theme playing on my playlist when I loaded the recent pages\
      \ for the first time\n\n'''Tarquin:''' I expect the spammer was running a bot.\
      \ :(\n\n'''EntropicLqd:''' Couldn't we update the Edit/Save page functionality\
      \ to randomly generate the name of the submission form and form elements and\
      \ store them on the user's session.  Then the update script could compare the\
      \ fields received with the names held on the session and if they don't match\
      \ then the update is not performed.  If the labels on the buttons were either\
      \ images (with generated names) or selected from a larger pool of possible labels\
      \ (e.g. Save, Update, Store Changes, you get the idea) then you'd effectively\
      \ disable a bot as it would have nothing to work with.  Hopefully I've explained\
      \ what I mean.\n\n'''El Muerte:''' Here's an implementation idea that would\
      \ stop bots. Add an hidden field that contains a hash of the page name and some\
      \ other stuff. When an edit is submitted the hash is checked to see if it's\
      \ correct. This way you always have to visit the edit page and can't use an\
      \ automated POST script. e.g. hash = md5(pageTitle+userAddr+secretKey) . When\
      \ the submission doesn't have the correct hash the edit page is popped up again\
      \ with your edited text. This way when you get a new IP assigned in the meanwhile\
      \ your edit won't be lost.\n\n'''Mychaeel:''' That's a pretty smart idea, I\
      \ think.  Right now most of the spam seems to be blocked by the link filter,\
      \ but I'll look into implementing the hash idea as well.\n\n----\n\n'''Mychaeel:'''\
      \ I've added code that automatically puts people who add too many links to a\
      \ page on read-only (with a \"low\" threshold which only causes a temporary\
      \ ban and a \"high\" one that causes an immediate permanent ban).  It also dispatches\
      \ an email to the wiki admin in those cases to ensure they can react in a timely\
      \ manner if necessary.\n\n'''Foxpaw:''' How many links is \"too many?\" It seems\
      \ like a person who was refactoring a page or merging two pages might add a\
      \ number of links in a single edit and trip the ban.\n\n'''Mychaeel:''' I've\
      \ currently defined 15 new external links as \"too many.\"  (Between 5 and 14\
      \ new external links, the user is temporarily put on read-only; no harm done\
      \ by that.  Just sit it out or wait until an admin undoes the auto-ban before\
      \ it times out.)  I doubt many pages even contain that number of external links.\n\
      \n'''Mychaeel:''' ...and already caught one tonight who tried to add 451 links\
      \ to [[Legacy:Brush Preservation|Brush Preservation]]...  :D\n\n'''Tarquin:'''\
      \ I don't think pages have that many external links. At most I usually see maybe\
      \ up to 5 in the External Links section. Most links on wiki pages are internal.\
      \ Good work Mych! :D\n\n'''strider:''' Brilliant idea Mychaeel!\n\n'''Mychaeel:'''\
      \ This afternoon, the spam trap caught a spammer who added the modest amount\
      \ of 12 links to [[Legacy:Using The Wiki|Using the Wiki]] and thus got temporarily\
      \ banned at first.  The automatic notification gave me the opportunity to make\
      \ the ban permanent and revert the changes within minutes.\n\n'''Mychaeel:'''\
      \ By the way, I'm getting down to business about the \"file an official complaint\"\
      \ part.  I've done so for the last two spammers, and we'll see what the response\
      \ is.\n\n'''Wormbo:''' I can't say which of these two good news I like better.\
      \ :D\n\n'''strider:''' You can now add rel=nofollow to anchor tags to make it\
      \ so external links aren't counted in search engine stats.  Wikipedia have already\
      \ added this feature, and doing this might help kill spam.\n\n'''Mychaeel:'''\
      \ Spammers don't spam because spam is effective; it's sufficient that somebody\
      \ ''tells them'' it is.  (That's especially evident in those spam attacks on\
      \ our wiki with code that didn't even render as links...)  Reducing the effectivity\
      \ of spam won't help, because the effectivity of spam is a non-issue to spammers.\
      \  They ''believe'' it's effective (or are made to by people who sell them spamming\
      \ services and software), and that's enough.  Spam has become a self-sufficient\
      \ phenomenon.\n\nThat said, the spam filter just caught another one adding 536\
      \ links to \"Inside The Death Chamber - Exploring Executions\" before any damage\
      \ could be done.  Funnily enough, we don't even have that page.\n:'''Wormbo:'''\
      \ I've removed the wiki link markup to \"Inside The Death Chamber - Exploring\
      \ Executions\" from your comment. Someone created that page only with the word\
      \ \"You\" on it. After I deleted it, it was created again with the same content.\n\
      \n'''Mychaeel:''' Caught three more in the course of the afternoon before they\
      \ could do damage.\n\n'''Mosquito:''' Have you considered passing this around\
      \ to other wikis, I'd imagine some are getting hit pretty hard these days.\n\
      \n'''Mychaeel:''' Actually, I pinched the basic idea for this filter from [[MeatBall:ShotgunSpam]],\
      \ so other wikis are probably already aware of this idea.  Our implementation\
      \ just adds auto-banning and email notifications.\n\n'''Mychaeel:''' One of\
      \ the ISPs I sent a complaint to has sent a reply:\n\n{{innerbox| Dear Sir/Madam,\n\
      \n \n\n We have already issued a warning to the user to ensure that such activity\
      \ is not repeated in future. Hence we would request you to consider this case\
      \ as closed. The Trouble Ticket Number for this complaint is '2973'.\n\n \n\n\
      \ Please do contact us if the incident repeats again.\n\n \n\n Assuring you\
      \ the best of our services.\n\n \n\n Thanking you.\n\n \n\n Yours Sincerely,\n\
      \n \n\n Antiabuse Support \n\n}}\n\nE-mail : Antiabuse.Support@relianceinfo.com\
      \ \n\nPhone : 91-(022) -30388464\n\n'''Mychaeel:''' ...and one more:\n\n{{innerbox|\
      \ Hi,\n\n \n\n we will take the necessary actions to stop this kind of illegal\
      \ activity.\n\n \n\n Ystävällisin terveisin \n\n}}\n\nBest Regards \n\nEUnet\
      \ Finland \n\npuh +358 9 4243 3205 \n\nfax +358 9 4243 0601 \n\nLinnoitustie\
      \ 4 B (Alto) \n\n02600 Espoo\n\n----\n\n'''EntropicLqd:''' Is the [[Legacy:Recent\
      \ Changes|Recent Changes]] page indexed?  It occured to me that the recent page\
      \ reverts were not so much an attempt at vandalising the Wiki page but merely\
      \ a way of getting a single link listed many times by placing the target link\
      \ in the change summary.  Might be worth preventing URLs from being placed in\
      \ the change description.\n\n'''Mychaeel:''' Pages whose URLs look like <code>http://wiki.beyondunreal.com/wiki?...</code>\
      \ are exempt from indexing, and after reversal, the spammy summary is removed\
      \ from [[Legacy:Recent Changes|Recent Changes]] too.  Of course, spammers wouldn't\
      \ even care about that subtlety if we put it on a flashing banner across the\
      \ editing page.\n\nPerhaps you're right with that idea, though I'm unclear about\
      \ why someone would want a plain-text URL spammed somewhere; it's not like any\
      \ search engine would consider that a \"link\" to increase the URL target's\
      \ PageRank.\n\nI could add a bit of code that prevents a page from being saved\
      \ (and redirects the user elsewhere to an explanation) when an URL is placed\
      \ in the summary field (and put a hint not to do that in the caption text below).\n\
      \nBy the same token, I'll probably extend the link counting algorithm to include\
      \ prior page revisions created by the same user as well.  I had anticipated\
      \ this development for a while &ndash; it's a well-established fact that spammers'\
      \ motivation to spam is not impaired by even the most coarse hints that it's\
      \ ultimately futile even if it manages to get past any technical measures, as\
      \ demonstrated by waves of spam emails crafted to bypass spam filters.  I just\
      \ hadn't been able to get myself to actually implement that.\n\n...which reminds\
      \ me of shooting off two complaints about those recent spammers.\n\n'''Birelli:'''\
      \ I think the last two are actually the same spammer. Two spams to a *.*.su\
      \ both for gambling (presumably) spamming in an unusual way, it's probably the\
      \ same one. And given that one IP is on the East Coast US, the other in Australia,\
      \ it's a fair bet that it's a redirect. \n\n----\n\n'''Joe@Chongqed:'''  Hi.\
      \  I saw your Temporary_Read-Only page.  I like the idea.  I added it to our\
      \ page discussing [http://wiki.chongqed.org//WikiSpam spam protection methods].\
      \  I am calling it AutoBan for now, do you have any better name?  You mentioned\
      \ above the idea comes from ShotGunSpam.  Is this method unique enough to deserve\
      \ a section of its own or should I just add to our description of ShotGunSpam?\n\
      \n----\n\n'''MythOpus:''' Awhile ago I caught a spammer who edited the actual\
      \ hyper link of links that were already on a few pages.  I don't think the auto\
      \ ban incorporates that so is there any way we can shut those spamming possibilities\
      \ down?  Also, it may be a little over the top but, a captcha system might be\
      \ a good idea.  Or, we can fake one :)  Make a fake little picture, put it on\
      \ the editing pages and spam bots would have to type in the special number.\
      \  If someone get's a spam bot in we can simply change the pic to something\
      \ else and change the pass?\n\n'''zugy:''' Hmmm-I wonder...It can't be too hard\
      \ to generate a security code thing, would it? You see them everywhere, these\
      \ days.... \n\n'''Draconx:''' Guah!  That temp ban thing raped me :(  I revert\
      \ the [[Legacy:Unreal Engine Versions|Unreal Engine Versions]] page which has\
      \ <b>twenty-six</b> external links on it and get owned for 4h 20m :(\n\n'''Mychaeel:'''\
      \ I had undone your temporary ban within minutes of getting the notification\
      \ email.  Thanks for the trouble.  :-)\n\n'''zugy:''' Bummer-I was gonna revert\
      \ that myself except- 1) didn't know how and 2) thought it might be an admin-only\
      \ kind of thing...\n\n----\n\n'''Guest:''' Mych, have you considered making\
      \ this page auto-update when a ban or other relevant event took place? I know\
      \ a feature like this wouldn't code itself, but it seems like you spend a lot\
      \ of time updating the table.\n\n'''MythOpus:''' Mych, are you have the time\
      \ of your life or what?\n\n'''Mychaeel:''' Actually, manually keeping track\
      \ of the auto-bans gets tiring after the first hundred or so.  I'd rather see\
      \ spammers discouraged or (gasp!) on their way of understanding that what they're\
      \ doing is wrong on so many levels.\n\n'''MythOpus:''' I'll be waiting for the\
      \ day when that happens.  Sadly, most spammers these days are getting paid big\
      \ bucks to do what they do.  It's a shame.  You should consider the auto-update\
      \ idea though.  I don't think it would be too hard to manage?\n\n'''Mychaeel:'''\
      \ Mostly I want to confirm that it's really a spamming attempt and not a false\
      \ positive before I log anything anywhere.\n\n'''T1:''' I have an idea that\
      \ may work to help slow down people from writing bots that change large amounts\
      \ of pages with only single links, therefore avoiding the autoban. You could\
      \ prevent non-registered users from doing many edits at once, by only allowing\
      \ 1 edit every 10 seconds. Basically you would store a global \"last edit time\"\
      \ and every time non-registered-user makes an edit, check if there has been\
      \ an edit in the last 10 seconds by a non-registered-user (it doesn't matter\
      \ if it's the same or a different one). If so, then you'd get a new page that\
      \ said, please wait X seconds, but it still has the textbox with what you wrote\
      \ so you don't lose your submission. This would prevent that situation some\
      \ referred to farther up the page where there was bot that was spamming faster\
      \ than wikizens could revert. The non-registered bot would be slower, but it\
      \ wouldn't affect non-registered users who aren't spamming very much. You might\
      \ say that the bot would then simply register before spamming, but you could\
      \ put a ten second limit on registering also, therefore each time it made an\
      \ edit, it would still be slower because the autoban or a good wikizen would\
      \ probably catch it. The only problem I forsee is people that don't register\
      \ and want to help against a bot, they may not be able to do much except slow\
      \ it down if they get a lucky shot into the minimal gap between the end of the\
      \ ten seconds and the next bot attack.\n\nAlso, if the ENTIRE text of the page\
      \ is deleted and replaced with more than two or three links and no non-links,\
      \ isn't that an obvious spam attempt that should be auto-detected, even if it's\
      \ less links than the standard autoban link limit?\n\n'''craze:''' i think the\
      \ Wiki just got a large dose of chongqing, tryed to see if i got my help desk\
      \ question answered, and woulkda ya look at tehh, massive page edits, all with\
      \ ad links in em'....\n\n'''T1:''' Another bot prevention method: If some one\
      \ adds the same text to more than two pages, autotempban. Happens a 3rd time?\
      \ it's an autopermaban. Now, if only someone would implement these things, it'd\
      \ slow dow/inconvience/prevent some spammers. Also, the statistics shouldn't\
      \ be on this page because then probably no one notices actual discussion on\
      \ this page because they think it's the stats being updated..\n\n'''El Muerte:'''\
      \ I think the spam check needs a fix to also scan for bad defined links. In\
      \ this case the bot should have gotten an tempban because he submitted 6 links,\
      \ but the links were not correct unrealwiki markup.\n\n'''Tarquin:''' I'm locking\
      \ the wiki for a bit.\n\n'''Wormbo:''' Hmm, all those changes came from different\
      \ IPs. A coordinated distributed spam attack? I think I smell a botnet.\n\n\
      '''Tarquin:''' I'm looking at our spam detection code, and Mych's use of regexps\
      \ is beyond me :(\n\n'''Mychaeel:''' The spam trap ''did'' spring &ndash; once\
      \ for each of the 41 different IP addresses the spammer used.\n\nCurrently,\
      \ the spam trap is very simple.  Yes, we can make it more sophisticated, but\
      \ in the end it'll always be just a small Perl script, while spammers will always\
      \ possess some level of human intelligence (even though the fact that this small\
      \ Perl script of ours manages to outsmart about 99% of them speaks volumes about\
      \ it).  So, while we can put huge amounts of brains into creating a script which\
      \ catches an even higher percentage of spammers, we'll still never reach the\
      \ point where we can be sure it'll catch ''all'' spammers.  I don't think we\
      \ can currently get any better effort/success ratio than we have already.\n\n\
      Several trusted wikizens here have been given a secret URL to lock the wiki\
      \ in case of emergency.  Why did nobody use it?\n\n'''T1:''' I know perl myself,\
      \ and I'd love to lend a hand in improving the script. In my opinion there is\
      \ nothing that cannot be improved upon, and I find programming much more fun\
      \ than wikignoming. 99% is still not 100%. Besides, my hard drive on my main\
      \ pc is messed up, and I have absolutely NOTHING to do right now, so I'd absolutely\
      \ love to do some semi-constructive programming.\n\nAlso, I'm checking for other\
      \ wikis that got hit by the same spammer, and I've found these so far:\n\nhttp://www.intertwingly.net/wiki/pie/FrontPage\n\
      \nhttp://wiki.python.org/moin/AtAudioSprint?action=diff\n\nhttp://wiki.43folders.com/index.php/Special:Recentchanges\n\
      \nhttp://wiki.kde.org/tiki-index.php?PHPSESSID=4942f9c1fedf2525cddb6c6e878ea769\n\
      \n'''Tarquin:''' Mych, my thought was actually to make the trap ''simpler''\
      \ &ndash; to check purely for URLs, rather than URL links. But if it caught\
      \ the spammer... cool :) BTW, what does '&#' do in the regexp? (line 4388)\n\
      \n'''T1:''' '&#'????? That's not even in the man page.\n\n:'''Mychaeel:''' It\
      \ looks for the string \"&#\" ;-) &ndash; those pattern matches are part of\
      \ the \"old\" spam detection code which was added when wikis were flooded by\
      \ \"chinese spam.\"  The auto-banning URL filter is somewhere else and indeed\
      \ simply looks for occurrences of <code>/UTF8_REPLACEMENThttp:/i</code>.\n\n\
      '''craze:''' although i know very little on the subject, how about having the\
      \ spam detection script(when tripped) begin to traceroute all edits within the\
      \ next x secconds/x minuites and check for matching IPs near the end of the\
      \ list, just to make sure its not just one person with a network of IPs  attacking\
      \ teh Wiki...(this probably makes no sence...)\n\n'''El Muerte:''' how about\
      \ using DNSBL to check the host? The wiki doesn't have that many edits so the\
      \ overhead might be low. Although in the last case it wouldn't have stopped\
      \ the sammer since he was using a zombie network.\n\n'''Switch`:''' How about\
      \ adding user registration and blocking anonymous posting? Maybe not exactly\
      \ in spirit of wiki but this is \"The Unreal Engine Documentation Site\" for\
      \ developers by developers first, wiki second.\n\n'''craze:''' ahh thats thwe\
      \ word i was thinking of, zombie, just check for a common IP twards the end\
      \ after it starts going through ISPs, just check if someone after the ISPs IP\
      \ has something after it, if i got this right, IPs twards the end of the list\
      \ would start to match(xept the last one) in the trace route, then if it starts\
      \ matching abd the anti spam scripts has been tripped just discard the edit\n\
      \n----\n\n'''MythOpus:''' Another spam attempt just occured.  The spammer mass\
      \ joined and logged in with several usernames and link spammed some pages. \
      \ Apparently, he thought he could trick us by deleting some of the links on\
      \ the pages as to make us think he deleted all of the links.  When will the\
      \ maturity level of these 'geniuses' level off with normal standards?\n\n'''Wormbo:'''\
      \ Did we already discuss these number/letter combinations presented through\
      \ an image and the user has to type it in? We could add something like this\
      \ to the edit page for each IP that didn't properly respond to an image yet.\
      \ After this verification the user or IP won't have to do it again. If verification\
      \ fails the user will only see the preview page. (so the edits aren't lost)\n\
      \n'''T1:''' Same spammer as last time.\n\nhttp://wiki.kde.org/\n\nThey had such\
      \ an image verification system, spammers hit them just as hard as us last time,\
      \ if not harder.\n\n----\n\n'''Wormbo:''' We need a way to revert pages with\
      \ many links. [[Legacy:Wiki Integration/Browser Sidebar|Wiki Integration/Browser\
      \ Sidebar]] got overridden with spam and attempts to revert it result in permanent\
      \ read-only, without any changes to the page.\n\n'''Mychaeel:''' Short-term\
      \ solutions: Admins are now exempt from the link threshold (should have been\
      \ that way from the start).  Long-term solution: Reverting a page to a previous\
      \ version should be allowed by anyone no matter what it does to the number of\
      \ links.\n\n----\n\n'''Mychaeel:''' That <code>display:</code><code> none</code>-type\
      \ spam is blocked now as well.\n\n----\n\n'''Tarquin:''' A lot of the links\
      \ here go to 404s on the Chongqing site.\n\n'''Kartoshka:''' 404s because whoever\
      \ added the links to the Chongqing page didn't submit them to chongqed.org?"
  properties: []
  revId: 2454
name: "Legacy:Chongqing Page/Discuss"
revision:
  revid: 2454
  parentid: 2456
  user: "Wormbo"
  timestamp: 1153205800.000000000
  comment: "reverted"
timestamp: 1668776917.322478000
