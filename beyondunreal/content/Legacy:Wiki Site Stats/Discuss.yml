---
parse:
  title: "Legacy:Wiki Site Stats/Discuss"
  text:
    text: "<p>Recently our Wiki site stats have dropped dramatically. At the present\
      \ this drop in page views and other stats is attributed to the lack of Unreal\
      \ Wiki results via the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.google.com\"\
      >Google</a> search engine. Possible reasons for this include:</p>\n<ul>\n<li>A\
      \ changed google response to Robots.txt</li>\n</ul>\n<p>(More will be added\
      \ as information is obtained.)</p>\n<h2><span class=\"mw-headline\" id=\"Solution_Discussion\"\
      >Solution Discussion</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\"\
      >[</span><a href=\"/edit/Legacy:Wiki_Site_Stats/Discuss?section=1\" title=\"\
      Edit section: Solution Discussion\">edit</a><span class=\"mw-editsection-bracket\"\
      >]</span></span></h2>\n<p><b>GRAF1K:</b> I personally think that completely\
      \ removing Robots.txt would be better overall for the site. At least as a temporary\
      \ measure until things get sorted out. At least then we would be positive that\
      \ Robots.txt is the problem.</p>\n<p><b>Mychaeel:</b> Currently no <a rel=\"\
      nofollow\" class=\"external text\" href=\"http://wiki.beyondunreal.com/robots.txt\"\
      >robots.txt</a> file is present for the Unreal Wiki, and we never set one up;\
      \ the usual search engine spiders are well-behaved enough not to put any excessive\
      \ stress on the servers, and the those private harvester tools that occasionally\
      \ grind the Wiki to a halt are very unlikely to pay any respect to the robots.txt\
      \ standard at all.</p>\n<p>However, about three weeks ago tarquin noticed that\
      \ there <i>was</i>, indeed, a robots.txt present in our root directory which\
      \ issued a blanket denial to all robots honoring the robots.txt standard, which\
      \ includes the GoogleBot. Consequently, Google removed any trace of all Unreal\
      \ Wiki pages from its index, and all that can be found when <a rel=\"nofollow\"\
      \ class=\"external text\" href=\"http://www.google.com/search?q=Unreal+Wiki\"\
      >googling for the Wiki</a> are links from other sites to it.</p>\n<p>We have\
      \ resubmitted the Unreal Wiki site address to Google after getting rid of the\
      \ misguided robots.txt in our root directory, but it takes a while until GoogleBot\
      \ gets around to reindexing any, let alone all pages of the Wiki. So whoever\
      \ instated that robots.txt we found in our root directory quite seriously harmed\
      \ the site, even though I don't suspect malice behind that – rather a misguided\
      \ attempt to keep badly behaved harvesters away.</p>\n<p><b>Wormbo:</b> There\
      \ is still a robots.txt in the www root allowing the google bot to only access\
      \ paths other than /wiki. Other useragents than google bot are even excluded\
      \ from the whole site.</p>\n<p><b>Tarquin:</b> Where is it?</p>\n<p><b>Mychaeel:</b>\
      \ Ah, you're talking about <a rel=\"nofollow\" class=\"external free\" href=\"\
      http://www.beyondunreal.com/robots.txt\">http://www.beyondunreal.com/robots.txt</a>.\
      \ The <a rel=\"nofollow\" class=\"external text\" href=\"http://www.robotstxt.org/wc/norobots.html\"\
      >Robot Exclusion Standard</a> specifies that a robot only looks for <code>/robots.txt</code>\
      \ on the same host it's trying to harvest, that is, wiki.beyondunreal.com (or\
      \ www.unrealwiki.com) in our case.</p>\n<p>The explicit exclusion of the <code>/wiki</code>\
      \ path on www.beyondunreal.com in that <code>robots.txt</code> is a bit surprising\
      \ indeed, though, especially given the fact that it's not a valid address anyway\
      \ (check <a rel=\"nofollow\" class=\"external free\" href=\"http://www.beyondunreal.com/wiki\"\
      >http://www.beyondunreal.com/wiki</a> to see yourself), let alone linked to\
      \ from anywhere.</p>\n<p><b>Tarquin:</b> So is there nothing else preventing\
      \ us from being back on Google? I submitted our URL a couple of times a few\
      \ weeks ago.</p>\n<p><b>Mychaeel:</b> No, there isn't. I resubmitted it myself\
      \ a week ago or so, but it may take a while until the GoogleBot picks it up.\
      \ I fear the bot has quite a packed schedule at this time...</p>\n<p><b>EricBlade:</b>\
      \ This whole section hasn't been updated in a year or so.. time to delete?</p>\n\
      \n<!-- \nNewPP limit report\nCPU time usage: 0.025 seconds\nReal time usage:\
      \ 0.086 seconds\nPreprocessor visited node count: 3/1000000\nPreprocessor generated\
      \ node count: 8/1000000\nPost‐expand include size: 0/2097152 bytes\nTemplate\
      \ argument size: 0/2097152 bytes\nHighest expansion depth: 2/40\nExpensive parser\
      \ function count: 0/100\n-->\n\n<!-- \nTransclusion expansion time report (%,ms,calls,template)\n\
      100.00%    0.000      1 - -total\n-->\n\n<!-- Saved in parser cache with key\
      \ wiki:pcache:idhash:3762-0!*!*!*!*!*!* and timestamp 20221118155408 and revision\
      \ id 9539\n -->\n"
  categories: []
  links: []
  templates: []
  images: []
  externallinks:
  - "http://www.google.com"
  - "http://www.beyondunreal.com/wiki"
  - "http://www.beyondunreal.com/robots.txt"
  - "http://www.google.com/search?q=Unreal+Wiki"
  - "http://wiki.beyondunreal.com/robots.txt"
  - "http://www.robotstxt.org/wc/norobots.html"
  sections:
  - toclevel: 1
    level: "2"
    line: "Solution Discussion"
    number: "1"
    index: "1"
    fromtitle: "Legacy:Wiki_Site_Stats/Discuss"
    byteoffset: 343
    anchor: "Solution_Discussion"
  displaytitle: "Legacy:Wiki Site Stats/Discuss"
  iwlinks: []
  wikitext:
    text: "Recently our Wiki site stats have dropped dramatically. At the present\
      \ this drop in page views and other stats is attributed to the lack of Unreal\
      \ Wiki results via the [http://www.google.com Google] search engine. Possible\
      \ reasons for this include:\n\n* A changed google response to Robots.txt\n\n\
      (More will be added as information is obtained.)\n\n==Solution Discussion==\n\
      \n'''GRAF1K:''' I personally think that completely removing Robots.txt would\
      \ be better overall for the site. At least as a temporary measure until things\
      \ get sorted out. At least then we would be positive that Robots.txt is the\
      \ problem.\n\n'''Mychaeel:''' Currently no [http://wiki.beyondunreal.com/robots.txt\
      \ robots.txt] file is present for the Unreal Wiki, and we never set one up;\
      \ the usual search engine spiders are well-behaved enough not to put any excessive\
      \ stress on the servers, and the those private harvester tools that occasionally\
      \ grind the Wiki to a halt are very unlikely to pay any respect to the robots.txt\
      \ standard at all.\n\nHowever, about three weeks ago tarquin noticed that there\
      \ ''was'', indeed, a robots.txt present in our root directory which issued a\
      \ blanket denial to all robots honoring the robots.txt standard, which includes\
      \ the GoogleBot.  Consequently, Google removed any trace of all Unreal Wiki\
      \ pages from its index, and all that can be found when [http://www.google.com/search?q=Unreal+Wiki\
      \ googling for the Wiki] are links from other sites to it.\n\nWe have resubmitted\
      \ the Unreal Wiki site address to Google after getting rid of the misguided\
      \ robots.txt in our root directory, but it takes a while until GoogleBot gets\
      \ around to reindexing any, let alone all pages of the Wiki.  So whoever instated\
      \ that robots.txt we found in our root directory quite seriously harmed the\
      \ site, even though I don't suspect malice behind that &ndash; rather a misguided\
      \ attempt to keep badly behaved harvesters away.\n\n'''Wormbo:''' There is still\
      \ a robots.txt in the www root allowing the google bot to only access paths\
      \ other than /wiki. Other useragents than google bot are even excluded from\
      \ the whole site.\n\n'''Tarquin:''' Where is it?\n\n'''Mychaeel:''' Ah, you're\
      \ talking about http://www.beyondunreal.com/robots.txt.  The [http://www.robotstxt.org/wc/norobots.html\
      \ Robot Exclusion Standard] specifies that a robot only looks for <code>/robots.txt</code>\
      \ on the same host it's trying to harvest, that is, wiki.beyondunreal.com (or\
      \ www.unrealwiki.com) in our case.\n\nThe explicit exclusion of the <code>/wiki</code>\
      \ path on www.beyondunreal.com in that <code>robots.txt</code> is a bit surprising\
      \ indeed, though, especially given the fact that it's not a valid address anyway\
      \ (check http://www.beyondunreal.com/wiki to see yourself), let alone linked\
      \ to from anywhere.\n\n'''Tarquin:''' So is there nothing else preventing us\
      \ from being back on Google? I submitted our URL a couple of times a few weeks\
      \ ago.\n\n'''Mychaeel:''' No, there isn't.  I resubmitted it myself a week ago\
      \ or so, but it may take a while until the GoogleBot picks it up.  I fear the\
      \ bot has quite a packed schedule at this time...\n\n'''EricBlade:''' This whole\
      \ section hasn't been updated in a year or so.. time to delete?"
  properties: []
  revId: 9539
name: "Legacy:Wiki Site Stats/Discuss"
revision:
  revid: 9539
  parentid: 9540
  user: "EricBlade"
  timestamp: 1143172675.000000000
  comment: "*"
timestamp: 1668786772.405798000
